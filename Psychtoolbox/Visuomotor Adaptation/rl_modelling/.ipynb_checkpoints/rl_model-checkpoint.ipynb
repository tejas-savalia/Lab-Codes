{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "## States: A continuous function of r and theta. Goal State: r = dist. theta = Theta\n",
    "## Actions: delta_r, delta_theta\n",
    "## Reward: -1 for every action. +100 to reach the goal state.\n",
    "## Transition Prob: s, a, s' = 1. s' changes based on condition\n",
    "### Environment takes a state action pair and returns reward and new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment: State = r, theta tuples. 1150*360\n",
    "#             Goal State: range of r and theta tuples.\n",
    "#             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = numpy.zeros((575, 360))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning rewards\n",
    "#hitting the circle\n",
    "\n",
    "environment[300, 40:50] = 10\n",
    "\n",
    "#On the straight trajectory towards the circle\n",
    "environment[:50, 44:46] = 5\n",
    "environment[50:100, 43:47] = 5\n",
    "environment[100:200, 42:48] = 5\n",
    "environment[200:300, 41:49] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(environment, state, action):\n",
    "    r, theta = state\n",
    "    del_r, del_theta = action\n",
    "    state = r+del_r, theta+del_theta\n",
    "    reward = environment[state[0], state[1]]\n",
    "    if reward == 10:\n",
    "        print ('Trial Complete')\n",
    "    return state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actions can be defined as change in angles. The change can be max 360 to min 0. del_r is assumed constant for the time being\n",
    "\n",
    "q_values = numpy.random.rand(575, 360, 360)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode complete\n"
     ]
    }
   ],
   "source": [
    "#Q Learning Algorithm\n",
    "#Terminal state q-values are 0. All others are random.\n",
    "del_r = 5\n",
    "q_values[300, 40:50, :] = 0\n",
    "episodes = 10\n",
    "for i in range(10):\n",
    "    state = numpy.random.choice(range(360)), numpy.random.choice(range(575))\n",
    "    for i in range(100):\n",
    "        del_theta = numpy.argmax(q_values[state[0], state[1], :])\n",
    "        #Take Action A (from set of actions a), observe R and S'\n",
    "        #Q(A, A) = Q(S, A) + alpha*[R + gamma*maxQ(S', a) - Q(S, A)]\n",
    "        #S <- S'\n",
    "    #Until S is terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State [1.73412574 1.14401065]\n",
      "Reward -1\n",
      "State [1.70107964 2.84684282]\n",
      "Reward -1\n",
      "State [3.89195469 3.27309234]\n",
      "Reward -1\n",
      "State [3.97719504 3.7848286 ]\n",
      "Reward -1\n",
      "State [4.42179457 3.47637942]\n",
      "Reward -1\n",
      "State [6.72377538 5.01214918]\n",
      "Reward -1\n",
      "State [7.63660932 6.34643123]\n",
      "Reward -1\n",
      "State [7.26553103 8.47780659]\n",
      "Reward -1\n",
      "State [7.30040833 7.33101064]\n",
      "Reward -1\n",
      "State [8.34070813 6.94698398]\n",
      "Reward -1\n",
      "State [9.90646266 8.42371838]\n",
      "Reward -1\n",
      "State [10.63437759  9.67605055]\n",
      "Reward -1\n",
      "State [12.5291473 11.2890919]\n",
      "Reward -1\n",
      "State [13.64155784 13.11996634]\n",
      "Reward -1\n",
      "State [14.92184679 13.47331926]\n",
      "Reward -1\n",
      "State [17.81237708 13.95413926]\n",
      "Reward -1\n",
      "State [17.85843516 14.23259037]\n",
      "Reward -1\n",
      "State [18.90960068 16.19596399]\n",
      "Reward -1\n",
      "State [20.69349737 16.64954004]\n",
      "Reward -1\n",
      "State [22.47270181 17.9854724 ]\n",
      "Reward -1\n",
      "State [23.66267024 18.72839406]\n",
      "Reward -1\n",
      "State [27.07925816 20.1117369 ]\n",
      "Reward -1\n",
      "State [28.02512654 22.32505754]\n",
      "Reward -1\n",
      "State [29.72110844 22.16878426]\n",
      "Reward -1\n",
      "State [29.27456933 22.87136   ]\n",
      "Reward -1\n",
      "State [29.61729643 24.00266926]\n",
      "Reward -1\n",
      "State [31.78865574 26.34365749]\n",
      "Reward -1\n",
      "State [32.73447735 28.10738399]\n",
      "Reward -1\n",
      "State [33.65362561 28.99232038]\n",
      "Reward -1\n",
      "State [33.55126654 29.40157262]\n",
      "Reward -1\n",
      "State [33.97077665 31.98901559]\n",
      "Reward -1\n",
      "State [34.29623852 32.75577837]\n",
      "Reward -1\n",
      "State [34.26759124 33.02215226]\n",
      "Reward -1\n",
      "State [34.33818236 33.80963749]\n",
      "Reward -1\n",
      "State [33.24634287 35.67192954]\n",
      "Reward -1\n",
      "State [32.11691626 36.30939413]\n",
      "Reward -1\n",
      "State [33.79665835 37.16686994]\n",
      "Reward -1\n",
      "State [33.92454299 38.35736496]\n",
      "Reward -1\n",
      "State [33.63323529 39.7282762 ]\n",
      "Reward -1\n",
      "State [34.68137392 42.65142998]\n",
      "Reward -1\n",
      "State [35.69706075 43.09134757]\n",
      "Reward -1\n",
      "State [36.59781537 41.13646544]\n",
      "Reward -1\n",
      "State [37.49577685 43.94897419]\n",
      "Reward -1\n",
      "State [38.54284864 44.4029138 ]\n",
      "Reward -1\n",
      "State [40.51370584 45.76075203]\n",
      "Reward -1\n",
      "State [41.22872533 45.91566111]\n",
      "Reward -1\n",
      "State [40.19697805 48.08503393]\n",
      "Reward -1\n",
      "State [40.30915393 49.36322413]\n",
      "Reward -1\n",
      "State [42.02386636 50.45851627]\n",
      "Reward -1\n",
      "State [41.61211504 52.00216824]\n",
      "Reward -1\n",
      "State [43.83135243 54.23011202]\n",
      "Reward -1\n",
      "State [44.17299145 56.81445636]\n",
      "Reward -1\n",
      "State [44.76459818 58.58582374]\n",
      "Reward -1\n",
      "State [47.43559607 59.61200864]\n",
      "Reward -1\n",
      "State [48.39150503 61.45206086]\n",
      "Reward -1\n",
      "State [51.54501183 62.2033949 ]\n",
      "Reward -1\n",
      "State [53.13916355 63.97999666]\n",
      "Reward -1\n",
      "State [53.78636691 66.06578025]\n",
      "Reward -1\n",
      "State [55.80775241 66.95915872]\n",
      "Reward -1\n",
      "State [57.32317171 68.64128672]\n",
      "Reward -1\n",
      "State [60.49263848 71.95904285]\n",
      "Reward -1\n",
      "State [61.09045005 72.85490359]\n",
      "Reward -1\n",
      "State [63.77955429 74.06735962]\n",
      "Reward -1\n",
      "State [63.79308026 77.12858353]\n",
      "Reward -1\n",
      "State [64.92630818 78.38819195]\n",
      "Reward -1\n",
      "State [64.75879702 80.72832431]\n",
      "Reward -1\n",
      "State [66.64007934 82.2083753 ]\n",
      "Reward -1\n",
      "State [67.80293862 83.85152862]\n",
      "Reward -1\n",
      "State [70.13957696 85.03892949]\n",
      "Reward -1\n",
      "State [71.02055059 85.68101414]\n",
      "Reward -1\n",
      "State [71.71408024 87.00273669]\n",
      "Reward -1\n",
      "State [71.13987547 89.21935731]\n",
      "Reward -1\n",
      "State [69.84773428 88.27613132]\n",
      "Reward -1\n",
      "State [71.61157909 89.33103558]\n",
      "Reward -1\n",
      "State [71.90860106 91.096466  ]\n",
      "Reward -1\n",
      "State [72.45970775 90.93108762]\n",
      "Reward -1\n",
      "State [72.92656118 91.4377498 ]\n",
      "Reward -1\n",
      "State [74.88342368 92.09240524]\n",
      "Reward -1\n",
      "State [74.6741171  93.94147465]\n",
      "Reward -1\n",
      "State [74.86152621 92.83431155]\n",
      "Reward -1\n",
      "State [77.55044589 92.93873754]\n",
      "Reward -1\n",
      "State [80.16359749 96.17109358]\n",
      "Reward -1\n",
      "State [79.81978051 96.89803358]\n",
      "Reward -1\n",
      "State [81.27235219 96.50413111]\n",
      "Reward -1\n",
      "State [81.3461828  97.61296476]\n",
      "Reward -1\n",
      "State [81.64058174 97.79099746]\n",
      "Reward -1\n",
      "State [81.97255634 98.05994808]\n",
      "Reward -1\n",
      "State [ 82.53036298 100.28009046]\n",
      "Reward -1\n",
      "State [ 84.08875114 100.76328772]\n",
      "Reward -1\n",
      "State [ 84.39621534 101.63220517]\n",
      "Reward -1\n",
      "State [ 87.08961999 101.06635365]\n",
      "Reward -1\n",
      "State [ 89.22504497 102.30880607]\n",
      "Reward -1\n",
      "State [ 89.04470144 103.57030873]\n",
      "Reward -1\n",
      "State [ 89.65823842 104.21172706]\n",
      "Reward -1\n",
      "State [ 92.37407454 106.7622392 ]\n",
      "Reward -1\n",
      "State [ 94.05041772 107.21900489]\n",
      "Reward -1\n",
      "State [ 95.01965051 108.57758422]\n",
      "Reward -1\n",
      "State [ 94.91637115 109.86795637]\n",
      "Reward -1\n",
      "State [ 96.88510933 109.59657101]\n",
      "Reward -1\n",
      "State [ 97.61473929 109.49577881]\n",
      "Reward -1\n"
     ]
    }
   ],
   "source": [
    "state = numpy.array([0, 0])\n",
    "for _ in range(100):\n",
    "    state, reward = step_environment(state, numpy.random.normal(1, 1, 2))\n",
    "    print (\"State\", state)\n",
    "    print (\"Reward\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
